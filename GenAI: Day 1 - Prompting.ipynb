{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6ef145a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-07T02:09:51.951154Z",
     "iopub.status.busy": "2025-04-07T02:09:51.950826Z",
     "iopub.status.idle": "2025-04-07T02:09:57.121195Z",
     "shell.execute_reply": "2025-04-07T02:09:57.120066Z"
    },
    "papermill": {
     "duration": 5.178399,
     "end_time": "2025-04-07T02:09:57.123164",
     "exception": false,
     "start_time": "2025-04-07T02:09:51.944765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -U -q \"google-genai==1.7.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88bd984f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:09:57.134211Z",
     "iopub.status.busy": "2025-04-07T02:09:57.133847Z",
     "iopub.status.idle": "2025-04-07T02:09:58.048794Z",
     "shell.execute_reply": "2025-04-07T02:09:58.047951Z"
    },
    "papermill": {
     "duration": 0.922061,
     "end_time": "2025-04-07T02:09:58.050415",
     "exception": false,
     "start_time": "2025-04-07T02:09:57.128354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "from IPython.display import HTML , Markdown , display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72ea1a6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:09:58.062049Z",
     "iopub.status.busy": "2025-04-07T02:09:58.061631Z",
     "iopub.status.idle": "2025-04-07T02:09:58.244920Z",
     "shell.execute_reply": "2025-04-07T02:09:58.244051Z"
    },
    "papermill": {
     "duration": 0.191155,
     "end_time": "2025-04-07T02:09:58.246689",
     "exception": false,
     "start_time": "2025-04-07T02:09:58.055534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "can_retry = lambda e: (isinstance(e , genai.errors.APIError) and e.code in {429 , 503})\n",
    "\n",
    "genai.models.Models.generate_content = retry.Retry(\n",
    "    predicate = can_retry)( genai.models.Models.generate_content\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6a42990",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:09:58.257020Z",
     "iopub.status.busy": "2025-04-07T02:09:58.256603Z",
     "iopub.status.idle": "2025-04-07T02:09:58.460293Z",
     "shell.execute_reply": "2025-04-07T02:09:58.459031Z"
    },
    "papermill": {
     "duration": 0.210482,
     "end_time": "2025-04-07T02:09:58.462060",
     "exception": false,
     "start_time": "2025-04-07T02:09:58.251578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38f81679",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:09:58.473070Z",
     "iopub.status.busy": "2025-04-07T02:09:58.472774Z",
     "iopub.status.idle": "2025-04-07T02:10:05.740482Z",
     "shell.execute_reply": "2025-04-07T02:10:05.739360Z"
    },
    "papermill": {
     "duration": 7.275111,
     "end_time": "2025-04-07T02:10:05.742325",
     "exception": false,
     "start_time": "2025-04-07T02:09:58.467214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = genai.Client(api_key = GOOGLE_API_KEY)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model = \"gemini-2.0-flash\",\n",
    "    contents = \"Tell me about Gemini 2.0 SDK ?\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10431174",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:10:05.752997Z",
     "iopub.status.busy": "2025-04-07T02:10:05.752693Z",
     "iopub.status.idle": "2025-04-07T02:10:05.758961Z",
     "shell.execute_reply": "2025-04-07T02:10:05.758154Z"
    },
    "papermill": {
     "duration": 0.012914,
     "end_time": "2025-04-07T02:10:05.760212",
     "exception": false,
     "start_time": "2025-04-07T02:10:05.747298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The term \"Gemini 2.0 SDK\" is a bit ambiguous, as there isn't a single official SDK directly labeled as \"Gemini 2.0.\"  However, it's most likely a reference to using the **Google AI Gemini models via the Google AI SDKs**.  Here's a breakdown of what that means and what you should know:\n",
       "\n",
       "**What is Google AI Gemini?**\n",
       "\n",
       "*   **Family of Models:** Gemini is a family of multimodal AI models developed by Google AI. Multimodal means they can process and understand various types of information like text, code, images, audio, and video.\n",
       "*   **Different Sizes:** Gemini comes in different sizes (Ultra, Pro, and Nano) to suit various use cases, from complex tasks to on-device applications.\n",
       "\n",
       "**The Google AI SDKs (Likely what's meant by \"Gemini 2.0 SDK\"):**\n",
       "\n",
       "*   **Access to Gemini:** The primary way to interact with Gemini models is through the Google AI SDKs.  Think of these SDKs as the *tools* that allow you to send instructions (prompts) to Gemini and receive responses.\n",
       "*   **Languages Supported:**  These SDKs are currently available in:\n",
       "    *   Python\n",
       "    *   Android (Kotlin/Java)\n",
       "    *   Web (JavaScript)\n",
       "    *   Dart (Flutter)\n",
       "    *   Swift\n",
       "\n",
       "*   **Functionality:** The SDKs provide methods to:\n",
       "    *   **Send Prompts:**  Formulate text-based or multimodal prompts for Gemini.\n",
       "    *   **Receive Responses:**  Get generated text, code, image interpretations, and more from the model.\n",
       "    *   **Manage Conversations:** Maintain conversational context for more natural interactions (using chat sessions).\n",
       "    *   **Configure Models:** Adjust parameters like temperature (randomness) and top-p (diversity) to fine-tune the model's output.\n",
       "    *   **Safety Settings:**  Control the sensitivity of the model to potentially harmful content.\n",
       "    *   **Streaming:**  Receive responses in real-time as they are generated, enhancing the user experience.\n",
       "\n",
       "**Key Features and Benefits of Using the Google AI SDKs with Gemini:**\n",
       "\n",
       "*   **Multimodal Capabilities:**  Leverage Gemini's ability to understand and generate content from various modalities (text, images, audio, etc.).\n",
       "*   **State-of-the-Art Performance:**  Access some of the most advanced AI models available, capable of tackling complex tasks.\n",
       "*   **Scalability and Reliability:**  Built on Google's infrastructure, ensuring your applications can handle demanding workloads.\n",
       "*   **Easy Integration:**  SDKs provide a straightforward way to integrate Gemini into your existing applications and workflows.\n",
       "*   **Customization:**  Fine-tune model parameters and safety settings to tailor Gemini's behavior to your specific needs.\n",
       "\n",
       "**How to Get Started (Replacing \"Gemini 2.0 SDK\" search):**\n",
       "\n",
       "1.  **Choose Your Language:** Select the SDK that corresponds to the programming language you're using (Python, Android, Web, Dart, Swift).\n",
       "2.  **Set Up Your Google Cloud Project:**  You'll need a Google Cloud project and API key to access the Gemini models.\n",
       "3.  **Install the SDK:**  Use your language's package manager (e.g., `pip` for Python, Gradle for Android) to install the relevant SDK.\n",
       "4.  **Authenticate:** Configure authentication to connect to the Google AI API using your API key.\n",
       "5.  **Write Code:**  Start using the SDK to send prompts to Gemini and process the responses.\n",
       "6.  **Explore Documentation:**  Refer to the official Google AI documentation for detailed information, code samples, and best practices.  Search Google for \"Google AI SDK [your language]\" (e.g., \"Google AI SDK Python\")\n",
       "\n",
       "**Example (Python):**\n",
       "\n",
       "```python\n",
       "import google.generativeai as genai\n",
       "\n",
       "# Configure the Gemini API key\n",
       "genai.configure(api_key=\"YOUR_API_KEY\")\n",
       "\n",
       "# Load the Gemini model (e.g., Gemini Pro)\n",
       "model = genai.GenerativeModel('gemini-pro')\n",
       "\n",
       "# Create a chat session (optional, for conversational apps)\n",
       "chat = model.start_chat(history=[])\n",
       "\n",
       "# Send a prompt to the model\n",
       "response = chat.send_message(\"Write a short poem about the ocean.\")\n",
       "\n",
       "# Print the generated text\n",
       "print(response.text)\n",
       "```\n",
       "\n",
       "**Important Considerations:**\n",
       "\n",
       "*   **Pricing:**  Using the Google AI API and Gemini models typically incurs costs. Review the pricing structure on the Google Cloud website.\n",
       "*   **Terms of Service:**  Be sure to comply with the Google AI Terms of Service and Acceptable Use Policy.\n",
       "*   **Responsible AI:**  Use the models responsibly and consider potential biases in the data.  Utilize safety settings provided in the SDK.\n",
       "*   **Model Updates:** Gemini and the underlying API are constantly evolving. Stay updated with the latest documentation and announcements from Google AI.\n",
       "\n",
       "**In summary:**  Instead of searching for a \"Gemini 2.0 SDK,\" focus on using the Google AI SDKs (Python, Android, Web, Dart, Swift) to access and interact with the Gemini models.  The SDKs provide the necessary tools and libraries to build AI-powered applications with text, images, and other modalities.  Be sure to consult the official Google AI documentation for the most up-to-date information and examples.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480203d5",
   "metadata": {
    "papermill": {
     "duration": 0.004273,
     "end_time": "2025-04-07T02:10:05.769494",
     "exception": false,
     "start_time": "2025-04-07T02:10:05.765221",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create a Chat structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47b5c83e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:10:05.779460Z",
     "iopub.status.busy": "2025-04-07T02:10:05.779123Z",
     "iopub.status.idle": "2025-04-07T02:10:06.471482Z",
     "shell.execute_reply": "2025-04-07T02:10:06.470365Z"
    },
    "papermill": {
     "duration": 0.699372,
     "end_time": "2025-04-07T02:10:06.473276",
     "exception": false,
     "start_time": "2025-04-07T02:10:05.773904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Tony! It's nice to meet you. How can I help you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat = client.chats.create(model = 'gemini-2.0-flash' , history = [])\n",
    "\n",
    "response = chat.send_message(\"Hello! My name is Tony\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a63114ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:10:06.486518Z",
     "iopub.status.busy": "2025-04-07T02:10:06.486248Z",
     "iopub.status.idle": "2025-04-07T02:10:10.562628Z",
     "shell.execute_reply": "2025-04-07T02:10:10.561396Z"
    },
    "papermill": {
     "duration": 4.084026,
     "end_time": "2025-04-07T02:10:10.563804",
     "exception": false,
     "start_time": "2025-04-07T02:10:06.479778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, Tony. Let's break down tariffs. To give you the most useful information, I need to know what you're interested in specifically. Generally speaking, tariffs can be a complicated topic, so I will need a bit more information to help you out.\n",
      "\n",
      "To get started, here's some general information about tariffs, and things to keep in mind:\n",
      "\n",
      "**What is a Tariff?**\n",
      "\n",
      "A tariff is essentially a tax or duty imposed by a government on goods or services imported from another country. They are usually calculated as a percentage of the value of the good (ad valorem tariff) or as a specific amount per unit (specific tariff).\n",
      "\n",
      "**Purpose of Tariffs:**\n",
      "\n",
      "*   **Protecting Domestic Industries:** This is a major reason. Tariffs increase the cost of imported goods, making them more expensive than domestically produced goods. This can give local industries a competitive advantage.\n",
      "*   **Raising Revenue:** Tariffs generate revenue for the government. While not always the primary goal, this income can be significant.\n",
      "*   **National Security:** Tariffs can be used to protect industries deemed vital to national security (e.g., defense, energy).\n",
      "*   **Retaliation:** Tariffs can be imposed in response to unfair trade practices by other countries (e.g., subsidies, dumping). This is a common feature of trade wars.\n",
      "*   **Encouraging Domestic Production:** Tariffs incentivize businesses to produce goods domestically, creating jobs and stimulating economic growth within the country.\n",
      "*   **Bargaining Chip:** Tariffs, or the threat of tariffs, can be used as a negotiating tactic in trade agreements.\n",
      "\n",
      "**Effects of Tariffs:**\n",
      "\n",
      "*   **Higher Prices for Consumers:** Tariffs increase the cost of imported goods, which often leads to higher prices for consumers.\n",
      "*   **Reduced Imports:** Tariffs make imported goods less competitive, leading to a decrease in imports.\n",
      "*   **Increased Domestic Production:** As imports decrease, domestic production may increase to meet demand.\n",
      "*   **Trade Wars:** Tariffs can escalate into trade wars if countries retaliate with their own tariffs. This can disrupt global trade and harm economies.\n",
      "*   **Impact on Businesses:** Businesses that rely on imported goods as inputs may face higher costs and reduced profits. Export-oriented businesses may suffer if other countries retaliate with tariffs on their products.\n",
      "*   **Potential Inefficiency:** Shielding domestic industries from competition can lead to reduced innovation and efficiency.\n",
      "\n",
      "**To give you more specific and helpful information, tell me what you are interested in:**\n",
      "\n",
      "*   **Are you interested in the tariffs imposed by a specific country (e.g., the US, China, the EU)?**\n",
      "*   **Are you interested in tariffs on a specific product (e.g., steel, aluminum, agricultural goods)?**\n",
      "*   **Are you interested in the *effects* of tariffs on a particular industry or economy?**\n",
      "*   **Are you interested in the *history* of tariffs, or a specific tariff dispute?**\n",
      "*   **Do you want to know about the *legal* aspects of tariffs, such as WTO rules?**\n",
      "*   **Are you interested in tariffs right now because of current events?**\n",
      "*   **What is your background knowledge about tariffs?**\n",
      "\n",
      "Once I know what you're trying to learn, I can provide more relevant information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Can you tell me more about the tariffs?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be3fd5ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:10:10.574831Z",
     "iopub.status.busy": "2025-04-07T02:10:10.574497Z",
     "iopub.status.idle": "2025-04-07T02:10:11.397266Z",
     "shell.execute_reply": "2025-04-07T02:10:11.396251Z"
    },
    "papermill": {
     "duration": 0.829885,
     "end_time": "2025-04-07T02:10:11.398764",
     "exception": false,
     "start_time": "2025-04-07T02:10:10.568879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, Tony. I remember your name is Tony.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Do you remember my name?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "581d8ab1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:10:11.409699Z",
     "iopub.status.busy": "2025-04-07T02:10:11.409458Z",
     "iopub.status.idle": "2025-04-07T02:10:11.579774Z",
     "shell.execute_reply": "2025-04-07T02:10:11.579041Z"
    },
    "papermill": {
     "duration": 0.176813,
     "end_time": "2025-04-07T02:10:11.580951",
     "exception": false,
     "start_time": "2025-04-07T02:10:11.404138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/aqa\n",
      "models/imagen-3.0-generate-002\n"
     ]
    }
   ],
   "source": [
    "for model in client.models.list():\n",
    "    print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e447077a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:10:11.591882Z",
     "iopub.status.busy": "2025-04-07T02:10:11.591622Z",
     "iopub.status.idle": "2025-04-07T02:10:11.753122Z",
     "shell.execute_reply": "2025-04-07T02:10:11.752191Z"
    },
    "papermill": {
     "duration": 0.168196,
     "end_time": "2025-04-07T02:10:11.754369",
     "exception": false,
     "start_time": "2025-04-07T02:10:11.586173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'Gemini 2.0 Flash',\n",
      " 'display_name': 'Gemini 2.0 Flash',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.0-flash',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_actions': ['generateContent', 'countTokens'],\n",
      " 'tuned_model_info': {},\n",
      " 'version': '2.0'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for model in client.models.list():\n",
    "    if model.name == \"models/gemini-2.0-flash\":\n",
    "        pprint(model.to_json_dict())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65835de6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:10:11.765254Z",
     "iopub.status.busy": "2025-04-07T02:10:11.764876Z",
     "iopub.status.idle": "2025-04-07T02:10:13.531157Z",
     "shell.execute_reply": "2025-04-07T02:10:13.530185Z"
    },
    "papermill": {
     "duration": 1.773074,
     "end_time": "2025-04-07T02:10:13.532448",
     "exception": false,
     "start_time": "2025-04-07T02:10:11.759374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The Double-Edged Sword: Examining the Impact of Tariffs on the US Economy\n",
      "\n",
      "Tariffs, taxes levied on imported goods, have been a recurring feature in the history of the US economy. They represent a powerful, yet often controversial, tool in the government's arsenal, wielded to protect domestic industries, generate revenue, and influence international trade relations. While proponents tout their potential to bolster national interests, critics highlight their potential to stifle economic growth and harm consumers. Understanding the complex and multifaceted impact of tariffs is crucial to navigating the increasingly interconnected global marketplace.\n",
      "\n",
      "One of the primary arguments in favor of tariffs is their ability to protect domestic industries from foreign competition. By increasing the price of imported goods, tariffs can level the playing field, allowing US manufacturers to compete more effectively, particularly in industries facing unfair practices like dumping or subsidized production. This protection can lead to job creation within the protected sectors, increased investment in domestic production, and a stronger national industrial base. Furthermore, tariffs can be\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "short_config = types.GenerateContentConfig(max_output_tokens = 200)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model = 'gemini-2.0-flash',\n",
    "    config = short_config,\n",
    "    contents = 'Write an essay about the impact of tariff\\'s on US economy'\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49abe1f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:10:13.543541Z",
     "iopub.status.busy": "2025-04-07T02:10:13.543323Z",
     "iopub.status.idle": "2025-04-07T02:10:15.773028Z",
     "shell.execute_reply": "2025-04-07T02:10:15.772161Z"
    },
    "papermill": {
     "duration": 2.237001,
     "end_time": "2025-04-07T02:10:15.774841",
     "exception": false,
     "start_time": "2025-04-07T02:10:13.537840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magenta\n",
      " -------------------------\n",
      "Magenta\n",
      " -------------------------\n",
      "Turquoise\n",
      " -------------------------\n",
      "Turquoise\n",
      " -------------------------\n",
      "Magenta\n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "high_temp_config = types.GenerateContentConfig(temperature = 2.0)\n",
    "\n",
    "for _ in range(5):\n",
    "    response = client.models.generate_content(\n",
    "        model = 'gemini-2.0-flash',\n",
    "        config = high_temp_config,\n",
    "        contents = 'Pick a random color... (respond in a single word)'\n",
    "    )\n",
    "    if response.text:\n",
    "        print(response.text , '-' * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d190448",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:10:15.787038Z",
     "iopub.status.busy": "2025-04-07T02:10:15.786699Z",
     "iopub.status.idle": "2025-04-07T02:10:18.184646Z",
     "shell.execute_reply": "2025-04-07T02:10:18.183652Z"
    },
    "papermill": {
     "duration": 2.405379,
     "end_time": "2025-04-07T02:10:18.185833",
     "exception": false,
     "start_time": "2025-04-07T02:10:15.780454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magenta\n",
      " -------------------------\n",
      "Magenta\n",
      " -------------------------\n",
      "Magenta\n",
      " -------------------------\n",
      "Magenta\n",
      " -------------------------\n",
      "Magenta\n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "low_temp_config = types.GenerateContentConfig(temperature = 0.0)\n",
    "\n",
    "for _ in range(5):\n",
    "    response = client.models.generate_content(\n",
    "        model = 'gemini-2.0-flash',\n",
    "        config = low_temp_config,\n",
    "        contents = 'Pick a random color... (respond in a single word)'\n",
    "    )\n",
    "    if response.text:\n",
    "        print(response.text , '-' * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de28475d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:10:18.197303Z",
     "iopub.status.busy": "2025-04-07T02:10:18.196983Z",
     "iopub.status.idle": "2025-04-07T02:10:22.337981Z",
     "shell.execute_reply": "2025-04-07T02:10:22.337042Z"
    },
    "papermill": {
     "duration": 4.148083,
     "end_time": "2025-04-07T02:10:22.339453",
     "exception": false,
     "start_time": "2025-04-07T02:10:18.191370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The Rust Belt Blues: Tariffs and the Tightening Squeeze\n",
      "\n",
      "The biting wind whipping off Lake Erie felt a lot like the economic chill that had settled over Erie, Pennsylvania. I stood outside the imposing, brick shell of what used to be the Peterson Steel Mill, a monument to American manufacturing. Now, weeds clawed through cracks in the concrete and the only movement was the creak of the wind through broken windows.\n",
      "\n",
      "\"Used to be the heartbeat of this town,\" said Frank Kowalski, a retired mill worker, his voice roughened by years of coal dust and frustration. He pointed a gnarled finger. \"Three shifts, 24/7. Good pay, good benefits. Now? Just a ghost.\"\n",
      "\n",
      "Frank, like many in Erie, blamed the recent tariffs for the mill's final death rattle. The story was simple, and brutally familiar across the Rust Belt. The US, in an attempt to protect domestic industries, had slapped tariffs on imported steel. The intention was noble, to bolster American steel production and bring jobs back home.\n",
      "\n",
      "But the ripple effect had been devastating.\n",
      "\n",
      "\"We used to import steel from Canada, good quality, fair price,\" Frank explained. \"Then the tariff hit. Peterson couldn't afford to compete. They tried to raise prices, but then the manufacturers using our steel, the auto companies, the construction companies, they started looking elsewhere.\"\n",
      "\n",
      "He was right. My investigation revealed that while some domestic steel producers initially saw a bump in profits, the long-term consequences were far more complex. The tariffs had increased the cost of raw materials for American manufacturers, making them less competitive on the global stage. Some companies, like Peterson Steel, simply couldn't keep up. Others shifted production overseas to bypass the tariffs, ironically contributing to the very job losses they were meant to prevent.\n",
      "\n",
      "Meanwhile, retaliatory tariffs from other countries hit American farmers hard. Soybean exports plummeted, leaving silos overflowing and farmers struggling to stay afloat. I spoke to Sarah Miller, a third-generation soybean farmer in Iowa, who had watched her livelihood crumble under the weight of tariffs.\n",
      "\n",
      "\"We supported the idea of standing up for American interests,\" she said, her voice heavy with disappointment. \"But the price we're paying is too high. We're losing our farms, our families are being torn apart. Is this really what winning looks like?\"\n",
      "\n",
      "The tariffs, intended to be a shield for American industries, had instead become a double-edged sword. While some sectors benefitted, the overall impact on the US economy was undeniably negative. The Rust Belt, once the engine of American prosperity, was now a stark reminder of the unintended consequences of protectionist policies. The debate rages on, but for Frank Kowalski and Sarah Miller, the damage is already done. The ghost of Peterson Steel Mill stands as a silent testament to the complex and often unpredictable impact of tariffs on the American economy. And the wind keeps blowing, carrying the whispers of broken promises and lost opportunities.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_config = types.GenerateContentConfig(\n",
    "    temperature = 1.0,\n",
    "    top_p = 0.95\n",
    ")\n",
    "\n",
    "story_prompt = \"You are a news reporter. Write a short story about the impact of tariff\\'s on US economy.\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model = 'gemini-2.0-flash',\n",
    "    config = model_config,\n",
    "    contents = story_prompt\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a58c0b8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:10:22.351618Z",
     "iopub.status.busy": "2025-04-07T02:10:22.351277Z",
     "iopub.status.idle": "2025-04-07T02:10:31.561231Z",
     "shell.execute_reply": "2025-04-07T02:10:31.560177Z"
    },
    "papermill": {
     "duration": 9.217956,
     "end_time": "2025-04-07T02:10:31.563284",
     "exception": false,
     "start_time": "2025-04-07T02:10:22.345328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_config = types.GenerateContentConfig(\n",
    "    temperature = 0.1,\n",
    "    top_p = 1,\n",
    "    max_output_tokens = 5\n",
    ")\n",
    "\n",
    "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
    "Review: \"Her\" is a disturbing study revealing the direction\n",
    "humanity is headed if AI is allowed to keep evolving,\n",
    "unchecked. I wish there were more movies like this masterpiece.\n",
    "Sentiment: \"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model = 'gemini-2.0-flash',\n",
    "    config = model_config,\n",
    "    contents = zero_shot_prompt\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f3f53f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:10:31.576550Z",
     "iopub.status.busy": "2025-04-07T02:10:31.576242Z",
     "iopub.status.idle": "2025-04-07T02:10:32.373632Z",
     "shell.execute_reply": "2025-04-07T02:10:32.372477Z"
    },
    "papermill": {
     "duration": 0.805469,
     "end_time": "2025-04-07T02:10:32.375341",
     "exception": false,
     "start_time": "2025-04-07T02:10:31.569872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "\n",
    "class Sentiment(enum.Enum):\n",
    "    POSITIVE = \"positive\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "    NEGATIVE = \"negative\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model = 'gemini-2.0-flash',\n",
    "    config = types.GenerateContentConfig(\n",
    "        response_mime_type = 'text/x.enum',\n",
    "        response_schema = Sentiment\n",
    "    ),\n",
    "    contents = zero_shot_prompt\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1aea6425",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:10:32.387705Z",
     "iopub.status.busy": "2025-04-07T02:10:32.387433Z",
     "iopub.status.idle": "2025-04-07T02:10:32.391913Z",
     "shell.execute_reply": "2025-04-07T02:10:32.390969Z"
    },
    "papermill": {
     "duration": 0.012092,
     "end_time": "2025-04-07T02:10:32.393302",
     "exception": false,
     "start_time": "2025-04-07T02:10:32.381210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment.POSITIVE\n",
      "<enum 'Sentiment'>\n"
     ]
    }
   ],
   "source": [
    "enum_response = response.parsed\n",
    "print(enum_response)\n",
    "print(type(enum_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "231f508d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:10:32.406030Z",
     "iopub.status.busy": "2025-04-07T02:10:32.405705Z",
     "iopub.status.idle": "2025-04-07T02:10:32.926967Z",
     "shell.execute_reply": "2025-04-07T02:10:32.925773Z"
    },
    "papermill": {
     "duration": 0.528996,
     "end_time": "2025-04-07T02:10:32.928324",
     "exception": false,
     "start_time": "2025-04-07T02:10:32.399328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\"size\": \"medium\",\n",
      "\"type\": \"normal\",\n",
      "\"ingredients\": [\"paneer\", \"olives\"]\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
    "\n",
    "EXAMPLE:\n",
    "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"small\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n",
    "}\n",
    "```\n",
    "\n",
    "EXAMPLE:\n",
    "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"large\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
    "}\n",
    "```\n",
    "\n",
    "ORDER:\n",
    "\"\"\"\n",
    "\n",
    "customer_order = \"Give me a medium pizza with paneer and olives\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model = 'gemini-2.0-flash',\n",
    "    config = types.GenerateContentConfig(\n",
    "        temperature = 0.1,\n",
    "        top_p = 1,\n",
    "        max_output_tokens = 250\n",
    "    ),\n",
    "    contents = [few_shot_prompt , customer_order]\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0684f947",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:10:32.940928Z",
     "iopub.status.busy": "2025-04-07T02:10:32.940655Z",
     "iopub.status.idle": "2025-04-07T02:10:33.814988Z",
     "shell.execute_reply": "2025-04-07T02:10:33.814181Z"
    },
    "papermill": {
     "duration": 0.882201,
     "end_time": "2025-04-07T02:10:33.816578",
     "exception": false,
     "start_time": "2025-04-07T02:10:32.934377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"size\": \"small\",\n",
      "  \"ingredients\": [\"pineapple\", \"figs\"],\n",
      "  \"type\": \"pizza\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import typing_extensions as typing\n",
    "\n",
    "class PizzaOrder(typing.TypedDict):\n",
    "    size: str\n",
    "    ingredients: list[str]\n",
    "    type: str\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model = 'gemini-2.0-flash',\n",
    "    config = types.GenerateContentConfig(\n",
    "        temperature = 0.1,\n",
    "        response_mime_type = \"application/json\",\n",
    "        response_schema = PizzaOrder\n",
    "    ),\n",
    "    contents = \"Can I have a small pizza with pineapple and figs\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "858e48a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:10:33.830112Z",
     "iopub.status.busy": "2025-04-07T02:10:33.829788Z",
     "iopub.status.idle": "2025-04-07T02:10:34.238986Z",
     "shell.execute_reply": "2025-04-07T02:10:34.237950Z"
    },
    "papermill": {
     "duration": 0.417444,
     "end_time": "2025-04-07T02:10:34.240497",
     "exception": false,
     "start_time": "2025-04-07T02:10:33.823053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
    "am 20 years old. How old is my partner? Return the answer directly.\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model = 'gemini-2.0-flash',\n",
    "    contents = prompt\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7323bf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:10:34.253329Z",
     "iopub.status.busy": "2025-04-07T02:10:34.252958Z",
     "iopub.status.idle": "2025-04-07T02:10:35.330388Z",
     "shell.execute_reply": "2025-04-07T02:10:35.329212Z"
    },
    "papermill": {
     "duration": 1.085269,
     "end_time": "2025-04-07T02:10:35.331720",
     "exception": false,
     "start_time": "2025-04-07T02:10:34.246451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's how to solve the problem step-by-step:\n",
       "\n",
       "1. **Find the age difference:** When you were 4, your partner was 3 times your age, so they were 4 * 3 = 12 years old.\n",
       "2. **Calculate the age difference:** The age difference between you and your partner is 12 - 4 = 8 years.\n",
       "3. **Calculate partner's current age:** Since the age difference remains constant, your partner is always 8 years older than you. Now that you are 20, your partner is 20 + 8 = 28 years old.\n",
       "\n",
       "**Therefore, your partner is currently 28 years old.**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
    "I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model = 'gemini-2.0-flash',\n",
    "    contents = prompt\n",
    ")\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d388d15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:10:35.344445Z",
     "iopub.status.busy": "2025-04-07T02:10:35.344136Z",
     "iopub.status.idle": "2025-04-07T02:10:35.348203Z",
     "shell.execute_reply": "2025-04-07T02:10:35.347500Z"
    },
    "papermill": {
     "duration": 0.01168,
     "end_time": "2025-04-07T02:10:35.349507",
     "exception": false,
     "start_time": "2025-04-07T02:10:35.337827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_instructions = \"\"\"\n",
    "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
    "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
    " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
    "     will return some similar entities to search and you can try to search the information from those topics.\n",
    " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
    "     so keep your searches short.\n",
    " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
    "\"\"\"\n",
    "\n",
    "example1 = \"\"\"Question\n",
    "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
    "\n",
    "Thought 1\n",
    "The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
    "\n",
    "Action 1\n",
    "<search>Milhouse</search>\n",
    "\n",
    "Observation 1\n",
    "Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
    "\n",
    "Thought 2\n",
    "The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
    "\n",
    "Action 2\n",
    "<lookup>named after</lookup>\n",
    "\n",
    "Observation 2\n",
    "Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
    "\n",
    "Thought 3\n",
    "Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
    "\n",
    "Action 3\n",
    "<finish>Richard Nixon</finish>\n",
    "\"\"\"\n",
    "\n",
    "example2 = \"\"\"Question\n",
    "What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
    "\n",
    "Thought 1\n",
    "I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
    "\n",
    "Action 1\n",
    "<search>Colorado orogeny</search>\n",
    "\n",
    "Observation 1\n",
    "The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
    "\n",
    "Thought 2\n",
    "It does not mention the eastern sector. So I need to look up eastern sector.\n",
    "\n",
    "Action 2\n",
    "<lookup>eastern sector</lookup>\n",
    "\n",
    "Observation 2\n",
    "The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
    "\n",
    "Thought 3\n",
    "The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
    "\n",
    "Action 3\n",
    "<search>High Plains</search>\n",
    "\n",
    "Observation 3\n",
    "High Plains refers to one of two distinct land regions\n",
    "\n",
    "Thought 4\n",
    "I need to instead search High Plains (United States).\n",
    "\n",
    "Action 4\n",
    "<search>High Plains (United States)</search>\n",
    "\n",
    "Observation 4\n",
    "The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
    "\n",
    "Thought 5\n",
    "High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
    "\n",
    "Action 5\n",
    "<finish>1,800 to 7,000 ft</finish>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed798a64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:10:35.363579Z",
     "iopub.status.busy": "2025-04-07T02:10:35.363246Z",
     "iopub.status.idle": "2025-04-07T02:10:35.996568Z",
     "shell.execute_reply": "2025-04-07T02:10:35.995617Z"
    },
    "papermill": {
     "duration": 0.641158,
     "end_time": "2025-04-07T02:10:35.997757",
     "exception": false,
     "start_time": "2025-04-07T02:10:35.356599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1\n",
      "I need to find the transformers NLP paper, then find the youngest author listed on the paper.\n",
      "\n",
      "Action 1\n",
      "<search>transformers NLP paper</search>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Question\n",
    "Who was the youngest author listed on the transformers NLP paper?\"\"\"\n",
    "\n",
    "react_config = types.GenerateContentConfig(\n",
    "    stop_sequences=[\"\\nObservation\"],\n",
    "    system_instruction = model_instructions + example1 + example2\n",
    ")\n",
    "\n",
    "react_chat = client.chats.create(\n",
    "    model = 'gemini-2.0-flash',\n",
    "    config = react_config\n",
    ")\n",
    "\n",
    "resp = react_chat.send_message(question)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45a254b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:10:36.010778Z",
     "iopub.status.busy": "2025-04-07T02:10:36.010444Z",
     "iopub.status.idle": "2025-04-07T02:10:36.967299Z",
     "shell.execute_reply": "2025-04-07T02:10:36.966214Z"
    },
    "papermill": {
     "duration": 0.965139,
     "end_time": "2025-04-07T02:10:36.968820",
     "exception": false,
     "start_time": "2025-04-07T02:10:36.003681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 2\n",
      "I need to find the youngest author listed on the paper. I will search each author's name to find their birthdate.\n",
      "\n",
      "Action 2\n",
      "<search>Ashish Vaswani</search>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "observation = \"\"\"Observation 1\n",
    "[1706.03762] Attention Is All You Need\n",
    "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
    "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
    "\"\"\"\n",
    "\n",
    "resp = react_chat.send_message(observation)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "271e573d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:10:36.984394Z",
     "iopub.status.busy": "2025-04-07T02:10:36.984101Z",
     "iopub.status.idle": "2025-04-07T02:10:44.695816Z",
     "shell.execute_reply": "2025-04-07T02:10:44.694890Z"
    },
    "papermill": {
     "duration": 7.721478,
     "end_time": "2025-04-07T02:10:44.697146",
     "exception": false,
     "start_time": "2025-04-07T02:10:36.975668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The youngest author listed on the \"Attention is All You Need\" paper (which introduced the Transformer architecture and is widely considered *the* foundational \"transformers NLP paper\") is likely **Aidan N. Gomez**.\n",
       "\n",
       "Here's why:\n",
       "\n",
       "* **Academic Stage:** At the time of the paper's publication in 2017, Aidan N. Gomez was a PhD student at the University of Toronto, working with Geoffrey Hinton.  Being a PhD student at the time of publication generally indicates a younger career stage compared to the other authors, most of whom were researchers at Google Brain.\n",
       "* **Affiliation:** His affiliation was listed as the University of Toronto and Vector Institute, while the others were primarily associated with Google Brain. This also suggests he was in a more junior academic position.\n",
       "\n",
       "While we don't have exact birth dates for all authors to definitively say who was *absolutely* the youngest, based on typical academic career paths and affiliations at the time of publication, **Aidan N. Gomez** is the most probable youngest author.\n",
       "\n",
       "It's worth noting that \"youngest\" is relative, and all the authors were accomplished researchers. However, in terms of career stage at the time of this seminal paper, Gomez was likely the most junior."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "from IPython.display import clear_output\n",
    "\n",
    "response = client.models.generate_content_stream(\n",
    "    model = 'gemini-2.0-flash-thinking-exp',\n",
    "    contents = 'Who was the youngest author listed on the transformers NLP paper?'\n",
    ")\n",
    "\n",
    "buf = io.StringIO()\n",
    "\n",
    "for chunk in response:\n",
    "    buf.write(chunk.text)\n",
    "    print(chunk.text , end = '')\n",
    "\n",
    "clear_output()\n",
    "Markdown(buf.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04b84b1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:10:44.710806Z",
     "iopub.status.busy": "2025-04-07T02:10:44.710524Z",
     "iopub.status.idle": "2025-04-07T02:10:45.354661Z",
     "shell.execute_reply": "2025-04-07T02:10:45.353721Z"
    },
    "papermill": {
     "duration": 0.65252,
     "end_time": "2025-04-07T02:10:45.356023",
     "exception": false,
     "start_time": "2025-04-07T02:10:44.703503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def factorial(n):\n",
       "  \"\"\"Calculates the factorial of a non-negative integer.\"\"\"\n",
       "  if n == 0:\n",
       "    return 1\n",
       "  else:\n",
       "    return n * factorial(n-1)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_prompt = \"\"\"\n",
    "Write a python function to calculate the factorial of a number. No explanation, just code\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model = 'gemini-2.0-flash',\n",
    "    config = types.GenerateContentConfig(\n",
    "        temperature = 1,\n",
    "        top_p = 1,\n",
    "        max_output_tokens = 1024\n",
    "    ),\n",
    "    contents = code_prompt\n",
    ")\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b416bc4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:10:45.369836Z",
     "iopub.status.busy": "2025-04-07T02:10:45.369558Z",
     "iopub.status.idle": "2025-04-07T02:10:47.389375Z",
     "shell.execute_reply": "2025-04-07T02:10:47.388163Z"
    },
    "papermill": {
     "duration": 2.02827,
     "end_time": "2025-04-07T02:10:47.390985",
     "exception": false,
     "start_time": "2025-04-07T02:10:45.362715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Okay, I can help with that. First, I will generate the first 14 odd '\n",
      "         'prime numbers, and then I will calculate their sum using Python.\\n'\n",
      "         '\\n'}\n",
      "----\n",
      "{'executable_code': {'code': 'def is_prime(n):\\n'\n",
      "                             '  \"\"\"\\n'\n",
      "                             '  Checks if a number is prime.\\n'\n",
      "                             '  \"\"\"\\n'\n",
      "                             '  if n <= 1:\\n'\n",
      "                             '    return False\\n'\n",
      "                             '  if n <= 3:\\n'\n",
      "                             '    return True\\n'\n",
      "                             '  if n % 2 == 0 or n % 3 == 0:\\n'\n",
      "                             '    return False\\n'\n",
      "                             '  i = 5\\n'\n",
      "                             '  while i * i <= n:\\n'\n",
      "                             '    if n % i == 0 or n % (i + 2) == 0:\\n'\n",
      "                             '      return False\\n'\n",
      "                             '    i += 6\\n'\n",
      "                             '  return True\\n'\n",
      "                             '\\n'\n",
      "                             'primes = []\\n'\n",
      "                             'num = 3 # Start with the first odd prime\\n'\n",
      "                             'while len(primes) < 14:\\n'\n",
      "                             '  if is_prime(num):\\n'\n",
      "                             '    primes.append(num)\\n'\n",
      "                             '  num += 2\\n'\n",
      "                             '\\n'\n",
      "                             \"print(f'{primes=}')\\n\"\n",
      "                             '\\n'\n",
      "                             'sum_of_primes = sum(primes)\\n'\n",
      "                             \"print(f'{sum_of_primes=}')\\n\",\n",
      "                     'language': 'PYTHON'}}\n",
      "----\n",
      "{'code_execution_result': {'outcome': 'OUTCOME_OK',\n",
      "                           'output': 'primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, '\n",
      "                                     '31, 37, 41, 43, 47]\\n'\n",
      "                                     'sum_of_primes=326\\n'}}\n",
      "----\n",
      "{'text': 'The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, '\n",
      "         '31, 37, 41, 43, and 47. Their sum is 326.\\n'}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "config = types.GenerateContentConfig(\n",
    "    tools = [types.Tool(code_execution = types.ToolCodeExecution())]\n",
    ")\n",
    "\n",
    "code_exec_prompt = \"\"\"\n",
    "Generate the first 14 odd prime numbers in Python, then calculate their sum\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model = 'gemini-2.0-flash',\n",
    "    config = config,\n",
    "    contents = code_exec_prompt\n",
    ")\n",
    "\n",
    "for part in response.candidates[0].content.parts:\n",
    "    pprint(part.to_json_dict())\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "994c60f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:10:47.404336Z",
     "iopub.status.busy": "2025-04-07T02:10:47.403985Z",
     "iopub.status.idle": "2025-04-07T02:10:47.413022Z",
     "shell.execute_reply": "2025-04-07T02:10:47.412315Z"
    },
    "papermill": {
     "duration": 0.016852,
     "end_time": "2025-04-07T02:10:47.414315",
     "exception": false,
     "start_time": "2025-04-07T02:10:47.397463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, I can help with that. First, I will generate the first 14 odd prime numbers, and then I will calculate their sum using Python.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "python \n",
       " def is_prime(n):\n",
       "  \"\"\"\n",
       "  Checks if a number is prime.\n",
       "  \"\"\"\n",
       "  if n <= 1:\n",
       "    return False\n",
       "  if n <= 3:\n",
       "    return True\n",
       "  if n % 2 == 0 or n % 3 == 0:\n",
       "    return False\n",
       "  i = 5\n",
       "  while i * i <= n:\n",
       "    if n % i == 0 or n % (i + 2) == 0:\n",
       "      return False\n",
       "    i += 6\n",
       "  return True\n",
       "\n",
       "primes = []\n",
       "num = 3 # Start with the first odd prime\n",
       "while len(primes) < 14:\n",
       "  if is_prime(num):\n",
       "    primes.append(num)\n",
       "  num += 2\n",
       "\n",
       "print(f'{primes=}')\n",
       "\n",
       "sum_of_primes = sum(primes)\n",
       "print(f'{sum_of_primes=}')\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\n",
       "sum_of_primes=326\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, and 47. Their sum is 326.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for part in response.candidates[0].content.parts:\n",
    "    if part.text:\n",
    "        display(Markdown(part.text))\n",
    "    elif part.executable_code:\n",
    "        display(Markdown(f'''python \\n {part.executable_code.code}\\n'''))\n",
    "    elif part.code_execution_result:\n",
    "        if part.code_execution_result.outcome != 'OUTCOME_OK':\n",
    "            display(Markdown(f'## Status {part.code_execution_result.outcome}'))\n",
    "\n",
    "        display(Markdown(f'```\\n{part.code_execution_result.output}\\n```'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8910a215",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T02:10:47.428234Z",
     "iopub.status.busy": "2025-04-07T02:10:47.427841Z",
     "iopub.status.idle": "2025-04-07T02:10:50.163244Z",
     "shell.execute_reply": "2025-04-07T02:10:50.162267Z"
    },
    "papermill": {
     "duration": 2.743567,
     "end_time": "2025-04-07T02:10:50.164564",
     "exception": false,
     "start_time": "2025-04-07T02:10:47.420997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This file is a Bash script designed to enhance your command-line prompt by displaying information about the current Git repository.  In other words, it adds Git status information to your prompt.\n",
       "\n",
       "**What it does:**\n",
       "\n",
       "*   **Displays Git status:** When you're working in a Git repository, it shows the branch you're on, whether you have uncommitted changes, if you're ahead or behind the remote branch, and other relevant details.\n",
       "*   **Customizable:** It allows you to customize the appearance of the Git information in your prompt using themes and symbols. You can change the colors, the information displayed, and how it's formatted.\n",
       "*   **Asynchronous updates:** It can fetch remote branch information in the background so that the prompt does not take a long time to load while you are waiting for the results of a git fetch.\n",
       "\n",
       "**Why you would use it:**\n",
       "\n",
       "*   **Improved workflow:**  Quickly see the status of your Git repository without having to run `git status` manually.\n",
       "*   **Contextual awareness:**  Provides immediate context about your current branch and the state of your changes.\n",
       "*   **Personalization:** Tailor the prompt to your liking, making it more informative and visually appealing.\n",
       "\n",
       "In essence, it's a utility that makes your command-line prompt Git-aware, increasing your efficiency when working with Git repositories.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n",
    "\n",
    "explain_prompt = f\"\"\"\n",
    "Please explain what this file does at a very high level. What is it, and why would I use it?\n",
    "```\n",
    "{file_contents}\n",
    "````\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model = 'gemini-2.0-flash',\n",
    "    contents = explain_prompt\n",
    ")\n",
    "\n",
    "Markdown(response.text)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 60.904324,
   "end_time": "2025-04-07T02:10:50.792385",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-07T02:09:49.888061",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
